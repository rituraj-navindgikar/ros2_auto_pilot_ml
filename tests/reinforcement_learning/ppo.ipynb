{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO - Proximal Policy Optimization \n",
    "State (Observations): Your LaserScan data (scan_0 to scan_359)\n",
    "\n",
    "Actions: Continuous cmd_vel values (linear_x and angular_z)\n",
    "\n",
    "Reward:\n",
    "  -  Positive: For reaching goals or following a clear path\n",
    "  -  Negative: For collisions, inefficiencies, or leaving boundaries\n",
    "\n",
    "Done Signal: Indicates when an episode ends (e.g., collision or reaching the goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import LaserScan\n",
    "from geometry_msgs.msg import Twist\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class RobotEnv(Node, Env):\n",
    "    def __init__(self):\n",
    "        super().__init__('robot_rl_env')\n",
    "\n",
    "        # Create a publisher for robot velocity commands\n",
    "        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)\n",
    "\n",
    "        # Subscribe to the LaserScan topic\n",
    "        self.create_subscription(LaserScan, '/scan', self.lidar_callback, 10)\n",
    "\n",
    "        # Initialize state and control variables\n",
    "        self.lidar_data = np.ones(360) * 10  # Initialize with max range\n",
    "        self.collision = False\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 500\n",
    "\n",
    "        # Action and observation space\n",
    "        self.action_space = Box(low=np.array([0.5, -0.6]), high=np.array([0.6, 0.6]), dtype=np.float64)  # [linear_x, angular_z]\n",
    "        self.observation_space = Box(low=0.0, high=10.0, shape=(360,), dtype=np.float64)  # 360-degree laser scan\n",
    "\n",
    "    def lidar_callback(self, msg: LaserScan):\n",
    "        \"\"\"Process LaserScan data.\"\"\"\n",
    "        self.lidar_data = np.array(msg.ranges)\n",
    "        self.lidar_data = np.clip(self.lidar_data, 0, 10)  # Clip ranges to max distance (10m)\n",
    "        if np.min(self.lidar_data) < 0.5:  # If very close to an obstacle\n",
    "            self.collision = True\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment.\"\"\"\n",
    "        self.get_logger().info('Environment reset')\n",
    "        self.collision = False\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Stop the robot\n",
    "        self._send_cmd_vel(0.0, 0.0)\n",
    "\n",
    "        # Allow sensors to stabilize\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Ensure the lidar data is float32\n",
    "        self.lidar_data = self.lidar_data.astype(np.float32)\n",
    "\n",
    "        # Return initial observation and additional info\n",
    "        return self.lidar_data, {}\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Perform a step in the environment.\"\"\"\n",
    "        linear_x, angular_z = action\n",
    "        self._send_cmd_vel(linear_x, angular_z)\n",
    "\n",
    "        # Wait for the action to complete\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        self.current_step += 1\n",
    "        reward = self._calculate_reward(action)\n",
    "        done = self._check_done()\n",
    "\n",
    "        # Ensure the lidar data is float32\n",
    "        self.lidar_data = self.lidar_data.astype(np.float32)\n",
    "\n",
    "        return self.lidar_data, reward, done, False, {}\n",
    "\n",
    "\n",
    "    def _send_cmd_vel(self, linear, angular):\n",
    "        \"\"\"Send velocity commands to the robot.\"\"\"\n",
    "        twist = Twist()\n",
    "        twist.linear.x = float(linear)  # Convert to Python float\n",
    "        twist.angular.z = float(angular)  # Convert to Python float\n",
    "        self.cmd_vel_pub.publish(twist)\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        \"\"\"Calculate the reward for the current step.\"\"\"\n",
    "        linear_x, angular_z = action\n",
    "        reward = 0\n",
    "        # Strong reward for moving forward\n",
    "\n",
    "        reward = linear_x * 50\n",
    "        \n",
    "        # Penalize over rotation\n",
    "        # reward -= abs(angular_z) * 2\n",
    "\n",
    "        # Reward rotation\n",
    "        reward += abs(angular_z) * 20\n",
    "\n",
    "        # Penalize longer episodes\n",
    "        reward -= self.current_step * 0.01\n",
    "\n",
    "        # Punish for collisions\n",
    "        if self.collision:\n",
    "            reward = -100\n",
    "        print(reward)\n",
    "        return reward\n",
    "\n",
    "    def _check_done(self):\n",
    "        \"\"\"Check if the episode is done.\"\"\"\n",
    "        if self.collision:\n",
    "            return True  # End the episode on collision\n",
    "        if self.current_step >= self.max_steps:\n",
    "            return True  # End the episode if max steps are reached\n",
    "        return False\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the environment.\"\"\"\n",
    "        self._send_cmd_vel(0.0, 0.0)  # Stop the robot\n",
    "\n",
    "    def render(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rclpy.init()\n",
    "\n",
    "# Initialize environment\n",
    "env = RobotEnv()\n",
    "\n",
    "# Check environment compatibility\n",
    "check_env(env)\n",
    "\n",
    "model = {}\n",
    "# Train PPO agent\n",
    "def train_rl():\n",
    "    # Create the PPO agent\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate=1e-3)\n",
    "            \n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=100000)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"ppo_robot_model\")\n",
    "    env.get_logger().info(\"Training complete and model saved.\")\n",
    "\n",
    "# Run training in a separate thread\n",
    "thread = threading.Thread(target=train_rl)\n",
    "thread.start()\n",
    "\n",
    "# Spin the ROS node\n",
    "rclpy.spin(env)\n",
    "\n",
    "# Shutdown\n",
    "env.close()\n",
    "rclpy.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_robot_model\")\n",
    "env.get_logger().info(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
